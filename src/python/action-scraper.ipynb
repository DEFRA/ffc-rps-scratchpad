{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gov.uk DEFRA Actions scraper\n",
    "\n",
    "We want to recover the full text of all the available actions on the DEFRA finder, so that we can analyse them and build up a taxonomy to help us derive a data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder_base_url = \"https://www.gov.uk/find-funding-for-land-or-farms\"\n",
    "page2 = \"?page=2\"\n",
    "page3 = \"?page=3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are threee pages of links so rather than spend time building a clever scraper we'll extract the links from all three pages, save them to files, manually trim the unnecessary ones and then concatenate. Then we can run through that list and pull the text of each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.iana.org/domains/example\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_hyperlinks(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all anchor tags\n",
    "    anchor_tags = soup.find_all('a')\n",
    "\n",
    "    # Extract the href attribute from each anchor tag\n",
    "    hyperlinks = [a.get('href') for a in anchor_tags if a.get('href')]\n",
    "\n",
    "    return hyperlinks\n",
    "\n",
    "# Example usage\n",
    "url = 'https://example.com'\n",
    "hyperlinks = extract_hyperlinks(url)\n",
    "\n",
    "# Print the extracted hyperlinks\n",
    "for link in hyperlinks:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get the three pages of links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_page1 = extract_hyperlinks(finder_base_url)\n",
    "\n",
    "with open('output/actions_links_page1.txt', 'w') as f:\n",
    "    for link in links_page1:\n",
    "        f.write(f\"{link}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_page2 = extract_hyperlinks(finder_base_url+page2)\n",
    "\n",
    "with open('output/actions_links_page2.txt', 'w') as f:\n",
    "    for link in links_page2:\n",
    "        f.write(f\"{link}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_page3 = extract_hyperlinks(finder_base_url+page3)\n",
    "\n",
    "with open('output/actions_links_page3.txt', 'w') as f:\n",
    "    for link in links_page3:\n",
    "        f.write(f\"{link}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we edit the files manually. Do that and then come back here : )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the three files\n",
    "\n",
    "with open('output/actions_links_page1.txt') as file:\n",
    "    links1 = file.readlines()\n",
    "with open('output/actions_links_page2.txt') as file:\n",
    "    links2 = file.readlines()\n",
    "with open('output/actions_links_page3.txt') as file:\n",
    "    links3 = file.readlines()\n",
    "\n",
    "# concatenate, stripping newlines\n",
    "\n",
    "all_links_relative = [link.rstrip() for link in links1 + links2 + links3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to iterate the list of links and retrieve each page. We'll store them in individual files and, if it looks easy enough, extract the codes and use them as keys in a dictionary to index into filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
